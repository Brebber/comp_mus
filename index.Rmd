---
title: "The Blurry Line between Rock and Metal"
author: "Brent Brakenhoff"
date: "`r Sys.Date()`"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    theme: 
      version: 5
      bootswatch: yeti
---
```{r setup, include=FALSE}
library(tidyverse)
library(spotifyr)
library(plotly)
library(compmus)
library(ggpubr)
metal <- get_playlist_audio_features("", "4jOlWW7XLRli6rjThfqlVl?si=781f9fafb8574da1")
rock <- get_playlist_audio_features("", "43rPmb2v9YWmbotTymQLQE?si=fd6f70a6a61045a3")
corpus <-
  bind_rows(
    rock |> mutate(genre = "Rock"),
    metal |> mutate(genre = "Metal")
  )
metal_stats <- metal |> 
  summarise(
    mean_valence = mean(metal$valence),
    mean_energy = mean(metal$energy),
    mean_dance = mean(metal$danceability),
    mean_tempo = mean(metal$tempo)
    )
rock_stats <- rock |> 
  summarise(
    mean_valence = mean(rock$valence),
    mean_energy = mean(rock$energy),
    mean_dance = mean(rock$danceability),
    mean_tempo = mean(rock$tempo)
    )

corp_stats <-
  bind_rows(
    rock_stats |> mutate(genre = "Rock"),
    metal_stats |> mutate(genre = "Metal")
)

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```
### Common Keys 
```{r hist}

hist <-  
  ggplot(corpus, aes(x=key_mode)) + 
  geom_histogram(stat="count", binwidth = 1, color = 'black', fill = 'red') +
  facet_wrap(~ genre) +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank()
        )

ggplotly(p=hist)
```
***
For every existent key, there is at least one song in the Rock playlist that uses it, this is not the case for the Metal playlist however. The popularity of Em among Metal tracks is unsurprising, as the key represents [grief, mournfulness and restlessness of spirit](https://interlude.hk/feel-key-e-minor/#:~:text=Pauer's%20key%20characteristics%20for%20E,of%20pieces%20in%20E%20minor.). For the Rock playlist there is not really a key that stands out as much.

### Songs with Low Valence (3/3)

```{r creep2}
creep <-
  get_tidy_audio_analysis("70LcF31zb1H0PyJoS1Sx1r?si=b753ddf24d3e430f") |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

c_chordogram <- creep |> 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  ggtitle("Chordogram")
ggplotly(p=c_chordogram)
```
***
(I'm not sure if I'm keeping this as a separate page or if I'm swapping this with the chromagram for Creep, as this is basically the same information.)\
As "Creep" has an interesting but simple chord progression, it's chordogram is clear. The four chords constantly looping are G, B, C and Cm and those are the only four chords in the song.

### Back to Keys (1/3)
```{r stwy}
stairway <-
  get_tidy_audio_analysis("5CQ30WqJwcep0pYcV4AMNc?si=682bdf162f4c4dcd") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

s_keygram <- stairway |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  ggtitle("Keygram")
ggplotly(p=s_keygram)
```

***
```{=html}
<iframe style="border-radius:20px"
src="https://open.spotify.com/embed/track/5CQ30WqJwcep0pYcV4AMNc?si=682bdf162f4c4dcd" width="100%" height="152" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>
```

A bit of a cliche, sure. 

### Back to Keys (2/3)

```{r mop}
master <-
  get_tidy_audio_analysis("2MuWTIM3b0YEAskbeeFE1i?si=f1b442b834af43b4") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

m_keygram <- master |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  ggtitle("Keygram")
ggplotly(p=m_keygram)
```

***
```{=html}
<iframe style="border-radius:20px"
src="https://open.spotify.com/embed/track/2MuWTIM3b0YEAskbeeFE1i?si=f1b442b834af43b4" width="100%" height="152" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>
```

Em

### Back to Keys (3/3)
```{r lot}
lotus <-
  get_tidy_audio_analysis("2ka0Oum3mNMGGXL9gBqdkq?si=d0cc5f6e3df641ba") |>
  compmus_align(sections, segments) |>
  select(sections) |>
  unnest(sections) |>
  mutate(
    pitches =
      map(segments,
          compmus_summarise, pitches,
          method = "mean", norm = "manhattan"
      )
  )

l_keygram <- lotus |> 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) |>
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "") +
  ggtitle("Keygram")
ggplotly(p=l_keygram)
```

*** 
```{=html}
<iframe style="border-radius:20px"
src="https://open.spotify.com/embed/track/2ka0Oum3mNMGGXL9gBqdkq?si=d0cc5f6e3df641ba" width="100%" height="152" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>
```

Unlike the previous two examples, it doesn't work for this song at all.

### Introducing My Taste in Music

```{=html}
<iframe style="border-radius:20px"
src="https://open.spotify.com/embed/playlist/09ybINQumucmE0BMdYakx2?" width="100%" height="152" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>
```

For this project I have chosen 10 bands, 5 of which are not considered Rock but not Metal bands and 5 of which are considered Metal bands. Metal is a subgenre of Rock that has a multitude of subgenres of itself. Metal is often faster, louder, more distorted and more extreme overall than Rock.  My goal is to see how easily distinguishable these genres actually are if we looked at the Spotify statistics.\

#### Band selection
The Rock bands I picked are: Queens of the Stone Age, Radiohead, Queen, Led Zeppelin and Arctic Monkeys. \
The Metal bands I picked are: Metallica, Death, Gojira, Opeth and TOOL.\
For both groups I tried to pick bands that sound as distinct as possible from eachother, however this list is definitely too limited to provide a complete overview of both genres. The non-Metal bands I chose are varied enough but still don't provide an overview of the genre in its entirety and 3 of the 5 metal bands I chose fall into different categories of the Death Metal subgenre.\

#### Song selection
For the rock songs I mainly focused on somewhat high tempo guitar based music, to make the distinction between Metal and Rock as hard as possible. Some Queen and Radiohead songs are obvious exceptions to this, however I feel these are still interesting to analyse as they often have shifting dynamics, or they can provide a baseline to define a non-Metal Rock song. Queens of the Stone Age is the non-Metal band that comes closest to being a Metal band, but I decided not to include their actual Metal songs. Led Zeppelin is often credited for at least partially inspiring the Metal genre.\
For the Metal playlist I picked songs that are unmistakably Metal. Metallica is an obvious inclusion, however I only added songs from their early Thrash Metal days. Death and Gojira fit the bill as well.\

#### Outliers
Queens of the Stone Age - 'My God is the Sun'; Heavy non-Metal song.\
Radiohead - 'Paranoid Android'; Alternates between soft Rock parts and chaotic loud solos.\
Led Zeppelin - 'Immigrant Song'; Considered "Proto-Metal", influential on the genre.\
Opeth - 'The Leper Affinity'; Good example of Opeth's dynamics.\
TOOL - 'Bottom' and 'Ticks & Leeches'; Both songs have very quiet build-up parts in the middle.

### Is Metal that Much Angrier?
```{r valen}
plot_1 <- corpus |>                    # Start with corpus.
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major"),
    name = corpus$track.name
  ) |>
  ggplot(                     # Set up the plot.
    aes(
      name = name,
      x = valence,
      y = energy,
      size = loudness,
      colour = mode
    )
  ) +
  geom_point() +              # Scatter plot.
  geom_rug(linewidth = 0.1) + # Add 'fringes' to show data distribution.

    geom_vline(aes(xintercept = mean_valence), corp_stats, color = "purple", linewidth = 0.2, show.legend = TRUE) +
  geom_hline(aes(yintercept = mean_energy), corp_stats, color = "purple", linewidth = 0.2, show.legend = TRUE) +

  facet_wrap(~ genre) +    # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Set1"        # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasise loud.
    guide = "none"            # Remove the legend for size.
  ) +
  theme_minimal() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Valence",
    y = "Energy",
    colour = "Mode"
  )
ggplotly(p = plot_1)
```
***

By looking at this plot it's obvious that the songs in the Metal part of the playlist are a lot more clustered toward high energy and low valence. The Rock songs seem to be a lot more scattered, which could be explained by the variety of Rock subgenres in that part of the playlist. The purple lines represent the means of both variables in both playlists and show a that Metal tends to have a lower valence and a higher energy. So yes, Metal is angrier (on average at least).\


### More Aggressive = Faster?
```{r dance}
plot_2 <- corpus |>                    # Start with corpus.
  mutate(
    name = corpus$track.name,
    duration = track.duration_ms,
    time_signature = ifelse(time_signature == 4, "Even", "Odd")
  ) |>
  ggplot(                     # Set up the plot.
    aes(
      name = name,
      x = danceability,
      y = tempo,
      size = duration,
      colour = time_signature
    )
  ) +
  geom_point() +              # Scatter plot.
  geom_rug(linewidth = 0.1) + # Add 'fringes' to show data distribution.
  geom_vline(aes(xintercept = mean_dance), corp_stats, color = "purple", linewidth = 0.2, show.legend = TRUE) +
  geom_hline(aes(yintercept = mean_tempo), corp_stats, color = "purple", linewidth = 0.2, show.legend = TRUE) +
  facet_wrap(~ genre) +    # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(60, 200),
    breaks = c(60, 130, 200),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Set1"        # Name of the palette is 'Paired'.
  ) +
  theme_minimal() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Danceability",
    y = "Tempo",
    colour = "Time signature",
    size = ''
  )
ggplotly(p = plot_2)
```
***

Contrary to my expectations, the mean tempo of the Metal playlist is actually slightly lower than the mean tempo of the Rock playlist.\

### Songs with Low Valence (1/3)
```{r tosir}
to_sirius <-
  get_tidy_audio_analysis("1BcuFfskHNf1WvqpyCs4wT?si=188253483ba54d55") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
plot_3 <- to_sirius |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
ggplotly(p=plot_3)
```

***
```{=html}
<iframe style="border-radius:20px"
src="https://open.spotify.com/embed/track/1BcuFfskHNf1WvqpyCs4wT?si=188253483ba54d55" width="100%" height="152" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>
```
This song has the lowest valence of not only the Metal part of the corpus, but of the corpus in its entirety. It also happens to have the highest energy of the entire corpus. The chromagram makes that pretty clear, by being almost entirely incomprehensible. The only pitch class which seems to be prevalent is the F. The parts at around 140 seconds and at 200 seconds stand out, as there are no real pitch classes that seem to be prevalent at all. In these parts the singing and the instrumental change a bit from the rest of the song, the singing making way for a low grunt and the palm muted guitars allow the drums to rise to the center of attention, muddling the chromagram.  

### Songs with Low Valence (2/3)
```{r creep}
creep <-
  get_tidy_audio_analysis("70LcF31zb1H0PyJoS1Sx1r?si=b7cfbb858fd645ac") |>
  select(segments) |>
  unnest(segments) |>
  select(start, duration, pitches)
plot_4 <- creep |>
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) |>
  compmus_gather_chroma() |> 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
ggplotly(p = plot_4)
```

***

```{=html}
<iframe style="border-radius:20px"
src="https://open.spotify.com/embed/track/70LcF31zb1H0PyJoS1Sx1r?si=b7cfbb858fd645ac" width="100%" height="152" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>
```
"Creep" by Radiohead scores lowest on valence in the Rock half of the corpus and the song scores a lot lower on energy than "To Sirius" on the previous page. Here the chord progression is a lot easier to read, as the G, B and C are the pitch classes with the highest magnitude.

### Songs with Weird Structures (1/2)

![](deliv.png)

***

```{=html}
<iframe style="border-radius:20px"
src="https://open.spotify.com/embed/track/4siXNiLG9VJR6Z2kP6fFjv?" width="100%" height="152" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>
```

The song "Deliverance" by Opeth is part of the Progressive Death Metal subgenre, which explains its peculiar structure. As visible in the cepstrogram the timbre changes at around a minute and 10 seconds in, when the vocalist starts singing instead of his usual low growling and more importantly the guitar tone changes from heavily distorted to clean. At roughly 5 minutes into the song, the clean singing starts again, however this part does feature a distorted guitar, so this is not visible in the cepstrogram. The self-similarity matrices highlight the same clean part as an outlier, and both show a highlight at around 10 minutes in at the introduction of the outro riff.

### Songs with Weird Structures (2/2)

![](paranoid.png)

***

```{=html}
<iframe style="border-radius:20px"
src="https://open.spotify.com/embed/track/6LgJvl0Xdtc73RJ1mmpotq?si=cb296db6ab774280" width="100%" height="152" frameBorder="0" allowtransparency="true" allow="encrypted-media" data-external="1"></iframe>
```

Another song in my corpus with an interesting structure is "Paranoid Android" by Radiohead. However where "Deliverance" has a clear standout moment, this song consists of three audibly different parts: the intro, the aggressive part and the "rain down" part (from around 3:35 to around 6:10). These different parts aren't clearly visible in the chroma based SSM unfortunately, as it shows so many significant changes it becomes a bit messy. These changes are visible in the cepstrogram and the timbre based SSM, however. This may have something to do with the fact the the song stays in the same key, but the instrumentation varies from an acoustic guitar in the intro and the "rain down" part to distorted guitars in the aggressive parts.